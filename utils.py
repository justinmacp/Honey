import json
import nltk.data
from unidecode import unidecode

'''
    uses json
    uses nltk.data
    result used in retrieve_wordcount
    retrieve_sentences(path) takes in the path to a json file that is structured as follows:
    
    {"collection":[{"rating": "78", "text": "This is a review."},...]}
    
    it returns 5 lists for each of the different ratings, neglecting any rating that is not an integer number from 1 to 5
'''
def retrieve_sentences(path):                                       #function to retrieve word statistics
    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')   #create an instance of the english tokenizer
    with open(path, 'r') as json_file:                              #open file
        data = json.load(json_file)['collection']                   #load file into data
#    list90 = list()                                                 #initialize list for sentences ranked in the 90s
#    list80 = list()                                                 #initialize list for sentences ranked in the 80s
#    list70 = list()                                                 #initialize list for sentences ranked in the 70s
#    list60 = list()                                                 #initialize list for sentences ranked in the 60s
#    list50 = list()                                                 #initialize list for sentences ranked in the 50s
    sentencedict = []                                               #create dictionary to store the output sentnces
    counter = 0                                                     #initialize counter to fill sentencedict
    for review in data:                                             #go through all collected reviews
        sentences = tokenizer.tokenize(review['review'])            #tokenize each review
        for i in sentences:                                         #go through tokens in review
            sentencedict.append({'sentence': i, 'rating': review['rating']})                  #add token
        
        '''
        if eval(review['rating']) > 90 :                            #if review is ranked 90 or above
            sentences = tokenizer.tokenize(review['review'])        #tokenize data into sentences
            for p in sentences:
                list90.append(p)                                    #add sentences to list of rank 90 sentences
        elif eval(review['rating']) > 80 :                          #if review is ranked in the 80s
            sentences = tokenizer.tokenize(review['review'])        #tokenize data into sentences
            for p in sentences:
                list80.append(p)                                    #add sentences to list of rank 80 sentences
        elif eval(review['rating']) > 70 :                          #if review is ranked in the 70s
            sentences = tokenizer.tokenize(review['review'])        #tokenize data into sentences
            for p in sentences:
                list70.append(p)                                    #add sentence to list of rank 70 sentences
        elif eval(review['rating']) > 60 :                          #if review is ranked in the 60s
            sentences = tokenizer.tokenize(review['review'])        #tokenize data into sentences
            for p in sentences:
                list60.append(p)                                    #add sentence to list of rank 60 sentences
        elif eval(review['rating']) <= 60 :                         #if review is ranked in the 50s
            sentences = tokenizer.tokenize(review['review'])        #tokenize data into sentences
            for p in sentences:
                list50.append(p)                                    #add sentence to list of rank 50 sentences
        '''

    return sentencedict                       #return dictionary

'''
    uses results from retrieve_sentences
    retrieve_wordcount(sentencelist) takes in a list that is generated by retrieve_sentences, i.e. this list will contain all sentences from a review in a list and it returns the individual words in all the sentences along with their total count in a dictionary
'''
def retrieve_wordcount(sentencelist):                       #function to retrieve wordcount
    wordlist = []                                           #create list to store words
    for sentence in sentencelist:                           #FOR LOOP (1): go through all the sentences in sentencelist
        words_in_p = sentence_cleanup(sentence['sentence'])     #clean up each sentence and return a list of words
        if not wordlist:                                        #if the wordlist is still empty
            wordlist.append({'word': words_in_p[0], 'local_data' :[{'count': 0, 'rating': sentence['rating']}], 'global_count': 0}) #add first word
        for word in words_in_p:                                 #FOR LOOP (2): go through all the words in the sentence
            word_element_exists = False                             #initialize boolean indicating existence of a word in the dictionary
            for element in wordlist:                                #FOR LOOP (3): go through all the words in the dictionary
                if element['word'] == word:                             #if the word exists in the dictionary
                    word_element_exists = True                          #set boolean to true indicating existence of word in dictionary
                    element['global_count'] +=1                         #increment its global count
                    local_rating_exists = False                         #initialize a boolean indicating wheither the local data for that word exists yet
                    for local_rating in element['local_data']:          #FOR LOOP (4): go through all the local ratings
                        if  local_rating['rating'] == sentence['rating']:   #if local rating already exists
                            local_rating['count'] += 1                      #increment the local count
                            local_rating_exists = True                      #set boolean to true indicating existence of local rating
                            break                                           #break out of loop (4)
                    if not local_rating_exists:                         #if local rating doesn't exist
                        element['local_data'].append({'count': 1, 'rating': sentence['rating']})    #add it to local data
            if not word_element_exists:                         #if word doesn't exist in dictionary
                wordlist.append({'word': word, 'local_data' :[{'count': 1, 'rating': sentence['rating']}], 'global_count': 1}) #add new entry
    return wordlist                                 #return the dictionary indicating word, count and rating
    
    '''
    #instantiate a dictionary object to give the number of times a word has been counted for each word
    dict = [{}]                     #instantiate the dictionary object
    for element in wordlist:        #go throught the wordlist
        if not element in dict:     #if word doesn't exist in dictionary
            dict[element] = 1       #add word to dictionary
        else:                       #if word already exists in dictionary
            dict[element] += 1      #increment count of word
    return dict                     #return dictionary
    '''

'''
    uses json
    wordbias(wordlist1,wordlist2,wordlist3,wordlist4,wordlist5) takes in dictionaries of words and their wordcounts as generated by retrieve_wordcount. wordbias will perform statistical analysis to check for dependence of a word to the ranking and it will output a dictionary that contains all dependent words with their calcuated bias. Optionally it will also return the expected ranking of a word and the standard deviation of that ranking
    returns list of dependent words in a json file
'''
def word_bias(wordlist,jsonfile):   #function to filter out words and give their biases
    wordbias = []
    dictionary = {}
    ################################ Using probabilistic calculations to calcuate dependence ################################
    
    
    for word in wordlist:  #iterate through wordlist1 since it has been augmented to contain all words in all 5 lists
        
        
        expected_value = 0.
        standard_deviation = 0.
        #EXPECTED VALUE FOR THE RATING OF A WORD
        for rating in word['local_data']:
            expected_value += eval(rating['rating'])*rating['count']
        expected_value /= word['global_count']
    
        #STANDARD DEVIATION OF THE RATING OF A WORD
        for rating in word['local_data']:
            standard_deviation += rating['count']*(eval(rating['rating'])-expected_value)**2
        standard_deviation = standard_deviation**0.5
        
        wordbias.append([word['word'], expected_value, standard_deviation])
    
    dictionary['dct'] = wordbias
    
    with open(jsonfile,'w') as json_file:       #open the json file to hold the dependent dictionary
        json.dump(dictionary, json_file)        #dump the dependent dictionary into the json file
    return wordbias

'''
    uses json
    sentence_filter takes in the path to the unfiltered json file and the path to a new json file which is to hold the output of this function. It also takes the list of dependent words. Comparing the dependent words to the words in the input file it removes any sentence that doesn't contain any dependent word. The output is dumped into the output json file
'''
def sentence_filter(inpath, outpath, dictpath):                     #function to remove noise sentences from a json file
    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')   #create an instance of the english tokenizer
    with open(dictpath, 'r') as json_dictfile:                      #open the dictionary of dependent words
        dep_dict = json.load(json_dictfile)['dct']                  #extract the dictionary
    with open(inpath, 'r') as json_infile:                          #open the input json file that contains the unfiltered review
        data = json.load(json_infile)                               #load data from input json file
    sentences = tokenizer.tokenize(data['review'])                  #tokenize data into sentences
    dep_sentences = []                                              #create list that will contain all dependent sentences
    for ia in sentences:                                            #FOR LOOP (1): go though all sentences in that file
        contains_dep = False                                        #boolean variable indicating wheither or not a sentence contains a dependent word
        wordlist = sentence_cleanup(ia)                             #extract words in a list for each of those sentences
        for word in wordlist:                                       #FOR LOOP (2): go though each of those words
            for dep_word in dep_dict:                               #FOR LOOP (3): go through the list of dependent words given in the function call
                if dep_word[2] < 5:                                 #if the word matches with a dependent word
                    contains_dep = True                             #set contains_dep to true
                    break                                           #break out of FOR LOOP (3) and go into FOR LOOP (2)
            if contains_dep:                                        #if a dependent word exists in the sentence
                break                                               #break out of FOR LOOP (2) and go into FOR LOOP (1)
        if contains_dep:                                            #if no dependent word exists in the sentence
            dep_sentences.append(ia)                                #remember that sentence
    json_output = {}
    json_output['result'] = dep_sentences
    with open(outpath, 'w') as json_outfile:                        #open the output json file to save the filtered data
        json.dump(json_output,json_outfile)                         #dump the filtered sentences into the output file

'''
    sentence_cleanup takes in a sentence and returns a list of the words contained in that sentence removing punctuation
'''
def sentence_cleanup(sentence):                     #function to extract the word list from a sentence
    wordlist = sentence.split()                     #split sentences into individual words
    wordlist = [x for x in wordlist if x != '-']    #remove all dashes from the wordlist
    for i in range(len(wordlist)):                  #iterate through all words to remove punctuation etc.
        newstring = wordlist[i]                     #place full word into a placeholder
    
        #check beginning of word for apostrophes etc.
        if not (newstring[0].isalnum()):    #if first character is not aphanumeric
            newstring = newstring[1:]       #remove it
        
        #check end of word for punctuation, elipses etc.
        if not (newstring[len(newstring)-1].isalnum()):         #in a 3x nested if statement check the last 3 characters
            newstring = newstring[:len(newstring)-1]            #if last char is a symbol remove it
            if not (newstring[len(newstring)-1].isalnum()):     #if second last char is a symbol
                newstring = newstring[:len(newstring)-1]        #remove it
                if not (newstring[len(newstring)-1].isalnum()): #if third last char is a symbol
                    newstring = newstring[:len(newstring)-1]    #remove it

        wordlist[i] = newstring.lower() #replace word from list with the cleaned up string in lowercase
    return wordlist                     #return the wordlist

















